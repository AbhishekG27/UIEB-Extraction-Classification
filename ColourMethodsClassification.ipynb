{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45d293-5d28-4e12-87c2-aca17c60f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Feature extraction methods (unchanged)\n",
    "def extract_dominant_color_descriptor(image):\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    kmeans.fit(pixels)\n",
    "    return kmeans.cluster_centers_.flatten()\n",
    "\n",
    "def extract_color_coherence_vector(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    hist = np.histogram(thresh, bins=256, range=[0, 256])[0]\n",
    "    return hist / np.sum(hist)\n",
    "\n",
    "def extract_color_layout_descriptor(image):\n",
    "    resized = cv2.resize(image, (8, 8))\n",
    "    return resized.flatten() / 255.0\n",
    "\n",
    "def extract_opponency_color_features(image):\n",
    "    b, g, r = cv2.split(image)\n",
    "    rg = r.astype(int) - g.astype(int)\n",
    "    yb = 0.5 * (r.astype(int) + g.astype(int)) - b.astype(int)\n",
    "    rg_hist = np.histogram(rg, bins=16, range=[-255, 255])[0]\n",
    "    yb_hist = np.histogram(yb, bins=16, range=[-255, 255])[0]\n",
    "    return np.concatenate([rg_hist, yb_hist]) / np.sum(np.concatenate([rg_hist, yb_hist]))\n",
    "\n",
    "def extract_color_name_descriptor(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1], None, [8, 8], [0, 180, 0, 256])\n",
    "    return hist.flatten() / np.sum(hist)\n",
    "\n",
    "def extract_hsv_histogram(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])\n",
    "    return hist.flatten() / np.sum(hist)\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_and_preprocess_images(folder_path, num_images=890, target_size=(224, 224)):\n",
    "    image_files = glob(os.path.join(folder_path, \"*.png\"))\n",
    "    random.shuffle(image_files)\n",
    "    images = []\n",
    "    \n",
    "    for file in image_files[:num_images]:\n",
    "        img = cv2.imread(file)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "            images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Define folders and corresponding methods\n",
    "folders = {\n",
    "    \"green_tint\": extract_dominant_color_descriptor,\n",
    "    \"blue_tint\": extract_dominant_color_descriptor,\n",
    "    \"red_tint\": extract_dominant_color_descriptor,\n",
    "    \"noisy\": extract_color_coherence_vector,\n",
    "    \"blurry\": extract_color_layout_descriptor,\n",
    "    \"hazy\": extract_opponency_color_features,\n",
    "    \"high_contrast\": extract_color_name_descriptor,\n",
    "    \"low_illumination\": extract_hsv_histogram\n",
    "}\n",
    "\n",
    "# Extract features for all folders\n",
    "print(\"Extracting features for all folders...\")\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for label, (folder, extraction_method) in enumerate(folders.items()):\n",
    "    folder_path = f\"C:/Users/abhis/Downloads/degraded_images/{folder}\"\n",
    "    images = load_and_preprocess_images(folder_path)\n",
    "    for img in images:\n",
    "        features = extraction_method(img)\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "\n",
    "# Pad feature vectors to have the same length\n",
    "max_length = max(len(f) for f in X)\n",
    "X_padded = np.array([np.pad(f, (0, max_length - len(f)), 'constant') for f in X])\n",
    "\n",
    "# Convert y to numpy array\n",
    "y = np.array(y)\n",
    "\n",
    "# Print shapes and unique classes for debugging\n",
    "print(f\"Shape of X_padded: {X_padded.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "unique_classes = np.unique(y)\n",
    "print(f\"Unique classes in the dataset: {unique_classes}\")\n",
    "print(f\"Number of unique classes: {len(unique_classes)}\")\n",
    "\n",
    "# Split the data\n",
    "print(\"Splitting the data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVC\": SVC(kernel='rbf'),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "print(\"Training and evaluating classifiers...\")\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Evaluating {name}...\")\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of {name}: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    target_names = list(folders.keys())\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(\"Classification process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d4ddc-d0e8-4844-af59-7095ae616ff3",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "* Accuracy of GaussianNB: 0.9792\n",
    "* Accuracy of KNeighborsClassifier: 0.9583\n",
    "* Accuracy of LogisticRegression: 0.9752\n",
    "* Accuracy of SVC: 0.8333\n",
    "* Accuracy of MLPClassifier: 0.9722\n",
    "* Accuracy of RandomForestClassifier: 1.0000\n",
    "* Accuracy of GradientBoostingClassifier: 1.0000\n",
    "* Accuracy of DecisionTreeClassifier: 0.8958\n",
    "* Accuracy of Bayes Classifier: 0.8712\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebae13e-3e51-4a7d-8283-7d6e70e0c1b7",
   "metadata": {},
   "source": [
    "\n",
    "**Output Explanation:**\n",
    "\n",
    "1. **GaussianNB: Accuracy of 0.9792**\n",
    "   - **Reason:** GaussianNB assumes normally distributed features and performs well with this dataset, resulting in high accuracy.\n",
    "\n",
    "2. **KNeighborsClassifier: Accuracy of 0.9583**\n",
    "   - **Reason:** KNN captures local data structures effectively but can be less accurate with noisy data.\n",
    "\n",
    "3. **LogisticRegression: Accuracy of 0.9752**\n",
    "   - **Reason:** Logistic Regression handles linearly separable data well, indicating distinct and well-separated features.\n",
    "\n",
    "4. **SVC: Accuracy of 0.8333**\n",
    "   - **Reason:** The RBF kernel in SVM is powerful but may overfit or not capture the data effectively, leading to lower accuracy.\n",
    "\n",
    "5. **MLPClassifier: Accuracy of 0.9722**\n",
    "   - **Reason:** MLP can model complex patterns in the data through its hidden layers, resulting in high accuracy.\n",
    "\n",
    "6. **RandomForestClassifier: Accuracy of 1.0000**\n",
    "   - **Reason:** Random Forests are robust and handle feature mix well, achieving perfect accuracy with enough trees.\n",
    "\n",
    "7. **GradientBoostingClassifier: Accuracy of 1.0000**\n",
    "   - **Reason:** Gradient Boosting sequentially improves predictions, leading to perfect accuracy.\n",
    "\n",
    "8. **DecisionTreeClassifier: Accuracy of 0.8958**\n",
    "   - **Reason:** Single Decision Trees may overfit the training data, resulting in lower accuracy compared to ensemble methods.\n",
    "\n",
    "9. **Bayesian Classifier: Accuracy of 0.8712**\n",
    "   - **Reason:** The Bayesian Classifier assumes a probabilistic model and can be sensitive to feature independence assumptions, resulting in lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dad7ac-a41d-4808-843b-62ce01a6416b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
