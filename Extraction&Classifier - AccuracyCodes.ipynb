{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c03884-33da-4002-929d-a8eae40bc146",
   "metadata": {},
   "source": [
    "### Performing Degradation Types on Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09870a-dd58-4672-a740-356830eb4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define the folder path containing the images and the output folder\n",
    "folder_path = r\"C:\\Users\\abhis\\Downloads\\images\"\n",
    "output_folder = r\"C:\\Users\\abhis\\Downloads\\degraded_images\"\n",
    "\n",
    "# Create subfolders for each effect\n",
    "effects = [\n",
    "    \"low_illumination\", \"high_contrast\", \"hazy\", \n",
    "    \"blurry\", \"noisy\", \"red_tint\", \"blue_tint\", \"green_tint\"\n",
    "]\n",
    "for effect in effects:\n",
    "    os.makedirs(os.path.join(output_folder, effect), exist_ok=True)\n",
    "\n",
    "# Get a list of all image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif'))]\n",
    "\n",
    "# Function to apply low illumination effect\n",
    "def degrade_low_illumination(image):\n",
    "    factor = 0.5  # Reduces brightness to 50%\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    return enhancer.enhance(factor), \"low_illumination\"\n",
    "\n",
    "# Function to apply high contrast effect\n",
    "def degrade_high_contrast(image):\n",
    "    factor = 2.0  # Increases contrast by a factor of 2\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    return enhancer.enhance(factor), \"high_contrast\"\n",
    "\n",
    "# Function to apply hazy effect\n",
    "def degrade_hazy(image):\n",
    "    image_array = np.array(image)\n",
    "    hazy_image = cv2.addWeighted(image_array, 0.5, np.ones(image_array.shape, image_array.dtype) * 255, 0.5, 0)\n",
    "    return Image.fromarray(hazy_image), \"hazy\"\n",
    "\n",
    "# Function to apply blurry effect\n",
    "def degrade_blurry(image):\n",
    "    image_array = np.array(image)\n",
    "    blurry_image = cv2.GaussianBlur(image_array, (15, 15), 0)\n",
    "    return Image.fromarray(blurry_image), \"blurry\"\n",
    "\n",
    "# Function to apply noisy effect\n",
    "def degrade_noisy(image):\n",
    "    image_array = np.array(image)\n",
    "    noise = np.random.normal(0, 25, image_array.shape).astype(np.uint8)\n",
    "    noisy_image = cv2.add(image_array, noise)\n",
    "    return Image.fromarray(noisy_image), \"noisy\"\n",
    "\n",
    "# Function to apply red tint effect\n",
    "def degrade_red_tint(image):\n",
    "    image_array = np.array(image)\n",
    "    red_tint = np.zeros_like(image_array)\n",
    "    red_tint[:, :, 0] = 100  # Adding red tint\n",
    "    red_tinted_image = cv2.add(image_array, red_tint)\n",
    "    return Image.fromarray(red_tinted_image), \"red_tint\"\n",
    "\n",
    "# Function to apply blue tint effect\n",
    "def degrade_blue_tint(image):\n",
    "    image_array = np.array(image)\n",
    "    blue_tint = np.zeros_like(image_array)\n",
    "    blue_tint[:, :, 2] = 100  # Adding blue tint\n",
    "    blue_tinted_image = cv2.add(image_array, blue_tint)\n",
    "    return Image.fromarray(blue_tinted_image), \"blue_tint\"\n",
    "\n",
    "# Function to apply green tint effect\n",
    "def degrade_green_tint(image):\n",
    "    image_array = np.array(image)\n",
    "    green_tint = np.zeros_like(image_array)\n",
    "    green_tint[:, :, 1] = 100  # Adding green tint\n",
    "    green_tinted_image = cv2.add(image_array, green_tint)\n",
    "    return Image.fromarray(green_tinted_image), \"green_tint\"\n",
    "\n",
    "# List of degradation functions\n",
    "degradation_functions = [\n",
    "    degrade_low_illumination,\n",
    "    degrade_high_contrast,\n",
    "    degrade_hazy,\n",
    "    degrade_blurry,\n",
    "    degrade_noisy,\n",
    "    degrade_red_tint,\n",
    "    degrade_blue_tint,\n",
    "    degrade_green_tint\n",
    "]\n",
    "\n",
    "# Apply all degradation effects to each image in the folder\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    original_image = Image.open(image_path)\n",
    "\n",
    "    for func in degradation_functions:\n",
    "        degraded_image, effect_name = func(original_image)\n",
    "        effect_folder = os.path.join(output_folder, effect_name)\n",
    "        degraded_image_path = os.path.join(effect_folder, f\"{os.path.splitext(image_file)[0]}_{effect_name}{os.path.splitext(image_file)[1]}\")\n",
    "        degraded_image.save(degraded_image_path)\n",
    "\n",
    "print(\"All images have been processed and saved with degradation effects.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6d2b4-f859-4088-82d9-076184bb2445",
   "metadata": {},
   "source": [
    "This Python script applies various image degradation effects to all image files in a specified folder and saves the processed images into subfolders organized by effect type. It first sets up the directory paths for the input and output, and creates subfolders for each type of degradation effect: low illumination, high contrast, hazy, blurry, noisy, and red, blue, and green tints. The script defines functions to apply each effect using the Python Imaging Library (PIL) and OpenCV. The effects are achieved by manipulating image brightness, contrast, applying Gaussian blur, adding random noise, and overlaying color tints. The script then iterates over each image in the input folder, applying all defined degradation functions, and saving the modified images into the corresponding subfolders within the output directory. Once processed, each image is saved with a filename indicating the applied effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c372d5e-7698-49d6-8245-a36995d4b08d",
   "metadata": {},
   "source": [
    "### 1. Feature Extractor using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe771aa4-93dd-47b6-9087-6226c7cc4e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define directories\n",
    "directories = [\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/green_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blue_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/red_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/noisy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blurry\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/hazy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/high_contrast\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/low_illumination\"\n",
    "]\n",
    "\n",
    "# Function to extract PCA color features\n",
    "def extract_pca_color_features(image, max_components=10):\n",
    "    # Convert to RGB and normalize\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pixels = image_rgb.reshape(-1, 3).astype(float) / 255.0\n",
    "    \n",
    "    # Calculate the maximum number of components\n",
    "    n_samples, n_features = pixels.shape\n",
    "    n_components = min(max_components, n_samples, n_features)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(pixels)\n",
    "    \n",
    "    # Pad the components and explained variance ratio if necessary\n",
    "    components = np.pad(pca.components_.flatten(), (0, max_components * 3 - len(pca.components_.flatten())))\n",
    "    explained_variance_ratio = np.pad(pca.explained_variance_ratio_, (0, max_components - len(pca.explained_variance_ratio_)))\n",
    "    \n",
    "    # Return both components and explained variance ratio\n",
    "    return np.concatenate((components, explained_variance_ratio))\n",
    "\n",
    "# List to hold all extracted features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "processed_images = 0\n",
    "\n",
    "for folder in directories:\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            # Read the image\n",
    "            image = cv2.imread(filepath)\n",
    "            if image is not None:\n",
    "                # Extract PCA features\n",
    "                pca_features = extract_pca_color_features(image)\n",
    "                all_features.append(pca_features)\n",
    "                all_labels.append(os.path.basename(folder))\n",
    "                processed_images += 1\n",
    "                print(f\"Processed {filename} successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to read {filename}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue  # Skip to the next image\n",
    "\n",
    "        # Print progress update\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if processed_images > 0 and elapsed_time > 0:\n",
    "            print(f\"Current processing speed: {processed_images / elapsed_time:.2f} images/sec\")\n",
    "        else:\n",
    "            print(\"Processing speed cannot be calculated yet.\")\n",
    "\n",
    "    print(f\"Finished processing folder: {folder}\")\n",
    "\n",
    "# Save the extracted features and labels\n",
    "np.save(\"PCAextractedcolor.npy\", np.array(all_features))\n",
    "np.save(\"PCA_labels.npy\", np.array(all_labels))\n",
    "\n",
    "print(\"Feature extraction complete.\")\n",
    "\n",
    "# Print 10 random images with their PCA features\n",
    "sample_indices = random.sample(range(len(all_features)), min(10, len(all_features)))\n",
    "for idx in sample_indices:\n",
    "    print(f\"Image: {all_labels[idx]}\")\n",
    "    print(f\"PCA Features: {all_features[idx]}\")\n",
    "\n",
    "    # Visualization (optional)\n",
    "    img_folder = [d for d in directories if os.path.basename(d) == all_labels[idx]][0]\n",
    "    img_files = os.listdir(img_folder)\n",
    "    if img_files:\n",
    "        img_path = os.path.join(img_folder, img_files[0])\n",
    "        plt.imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"PCA Features for {os.path.basename(img_path)}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images found in {img_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01157869-bf00-4135-9f89-f7cc60a70492",
   "metadata": {},
   "source": [
    "### Classifier Using PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33707bed-7f3f-4c4f-ab87-e32aa01db56e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load features and labels\n",
    "features = np.load(\"PCAextractedcolor.npy\")\n",
    "labels = np.load(\"PCA_labels.npy\")\n",
    "\n",
    "# Check the unique labels to ensure they are correct\n",
    "print(\"Unique labels:\", np.unique(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Define classifiers with hyperparameter grids\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": (GaussianNB(), {}),\n",
    "    \"K-Nearest Neighbors\": (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7, 9]}),\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=1000), {'classifier__C': [0.1, 1, 10]}),\n",
    "    \"SVM\": (SVC(), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['rbf', 'linear']}),\n",
    "    \"MLP\": (MLPClassifier(max_iter=1000), {'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)]}),\n",
    "    \"Random Forest\": (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 200]}),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(), {'classifier__n_estimators': [50, 100, 200], 'classifier__learning_rate': [0.01, 0.1]}),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {'classifier__max_depth': [5, 10, None]})\n",
    "}\n",
    "\n",
    "# Create a pipeline with feature selection, scaling, and classifier\n",
    "def create_pipeline(clf):\n",
    "    return Pipeline([\n",
    "        ('feature_selection', SelectKBest(f_classif, k=20)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "best_classifiers = {}\n",
    "\n",
    "for name, (clf, param_grid) in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipeline = create_pipeline(clf)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    best_classifiers[name] = (grid_search.best_estimator_, accuracy)\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, clf) for name, (clf, _) in best_classifiers.items()],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "\n",
    "print(\"\\nVoting Classifier Results:\")\n",
    "print(f\"Accuracy: {voting_accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred_voting))\n",
    "\n",
    "# Find the best individual classifier\n",
    "best_classifier = max(best_classifiers.items(), key=lambda x: x[1][1])\n",
    "print(f\"\\nBest individual classifier: {best_classifier[0]} with accuracy {best_classifier[1][1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbccb5-d26b-4f4d-b86e-acfde2957b28",
   "metadata": {},
   "source": [
    "PCA extracted\n",
    "\n",
    "* Gaussian Naive Bayes accuracy: 0.35\n",
    "* K-Nearest Neighbors accuracy: 0.38\n",
    "* Logistic Regression accuracy: 0.41\n",
    "* SVM accuracy: 0.45\n",
    "* MLP accuracy: 0.48\n",
    "* Random Forest accuracy: 0.45\n",
    "* Gradient Boosting accuracy: 0.47\n",
    "* Decision Tree accuracy: 0.43\n",
    "* Bayes accuracy : 0.32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa11d29-b355-47e8-9857-7b20a66045d5",
   "metadata": {},
   "source": [
    "\n",
    "Output Explanation\n",
    "\n",
    "Gaussian Naive Bayes (35% accuracy): This classifier assumes feature independence, which might not hold well with PCA features as they capture complex patterns. The low accuracy suggests that this assumption limits its performance.\n",
    "\n",
    "K-Nearest Neighbors (38% accuracy): KNN's performance is better but still low because it relies heavily on the notion of proximity in feature space, which can be distorted with PCA-transformed data.\n",
    "\n",
    "Logistic Regression (41% accuracy): Logistic regression provides a moderate accuracy by using a linear decision boundary. The accuracy indicates some capability in handling PCA features but still struggles with non-linear patterns.\n",
    "\n",
    "SVM (45% accuracy): Support Vector Machine offers a higher accuracy by finding optimal hyperplanes, which helps it capture more complex relationships in the data than linear classifiers.\n",
    "\n",
    "MLP (48% accuracy): The Multi-Layer Perceptron, a type of neural network, performs relatively well as it can model non-linear relationships in the data, benefiting from deeper network architectures.\n",
    "\n",
    "Random Forest (45% accuracy): Random Forest performs similarly to SVM by using ensemble learning to handle complex feature interactions, but it might still struggle with high-dimensional data like PCA features.\n",
    "\n",
    "Gradient Boosting (47% accuracy): This method incrementally improves model predictions and manages to capture intricate patterns in the data, leading to better performance compared to simpler models.\n",
    "\n",
    "Decision Tree (43% accuracy): While simple and interpretable, Decision Trees have lower accuracy because they are prone to overfitting, especially in high-dimensional spaces without ensemble techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352b55d-8ca5-4797-b5f4-846b7f022b9f",
   "metadata": {},
   "source": [
    "### 2. Feature Extractor using HOG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af609e05-0aa2-4f12-8f81-ce0635144721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define directories\n",
    "directories = [\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/green_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blue_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/red_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/noisy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blurry\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/hazy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/high_contrast\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/low_illumination\"\n",
    "]\n",
    "\n",
    "# Function to extract HOG color features\n",
    "def extract_hog_color_features(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    image_resized = cv2.resize(image, (128, 128))\n",
    "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    hog_features = []\n",
    "    for channel in range(3):  # Extract HOG for each color channel\n",
    "        channel_hog = hog(\n",
    "            image_rgb[:,:,channel], \n",
    "            pixels_per_cell=pixels_per_cell,\n",
    "            cells_per_block=cells_per_block,\n",
    "            orientations=orientations,\n",
    "            visualize=False\n",
    "        )\n",
    "        hog_features.extend(channel_hog)\n",
    "    \n",
    "    return np.array(hog_features)\n",
    "\n",
    "# List to hold all extracted features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "processed_images = 0\n",
    "\n",
    "for folder in directories:\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            # Read the image\n",
    "            image = cv2.imread(filepath)\n",
    "            if image is not None:\n",
    "                # Extract HOG features\n",
    "                hog_features = extract_hog_color_features(image)\n",
    "                all_features.append(hog_features)\n",
    "                all_labels.append(os.path.basename(folder))  # Use folder name as label\n",
    "                processed_images += 1\n",
    "                print(f\"Processed {filename} successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to read {filename}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        # Print progress update\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time > 0:\n",
    "            print(f\"Current processing speed: {processed_images / elapsed_time:.2f} images/sec\")\n",
    "\n",
    "    print(f\"Finished processing folder: {folder}\")\n",
    "\n",
    "# Save the extracted features and labels\n",
    "np.save(\"HOGextractedcolor.npy\", np.array(all_features))\n",
    "np.save(\"HOG_labels.npy\", np.array(all_labels))\n",
    "\n",
    "print(\"Feature extraction complete.\")\n",
    "\n",
    "# Print 10 random images with their HOG features\n",
    "sample_indices = random.sample(range(len(all_features)), min(10, len(all_features)))\n",
    "for idx in sample_indices:\n",
    "    print(f\"Image: {all_labels[idx]}\")\n",
    "    print(f\"HOG Features shape: {all_features[idx].shape}\")\n",
    "    print(f\"First few HOG Features: {all_features[idx][:10]}\")\n",
    "\n",
    "    # Visualization (optional)\n",
    "    img_folder = [d for d in directories if os.path.basename(d) == all_labels[idx]][0]\n",
    "    img_files = os.listdir(img_folder)\n",
    "    if img_files:\n",
    "        img_path = os.path.join(img_folder, img_files[0])\n",
    "        plt.imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"HOG Features for {os.path.basename(img_path)}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images found in {img_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e65f2-e46d-44eb-8a3c-0d9a78397e42",
   "metadata": {},
   "source": [
    "### Classifier for HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04048491-2bfe-480c-90a7-01c8ea39d347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "# Load features and labels\n",
    "print(\"Loading features and labels...\")\n",
    "features = np.load(\"HOGextractedcolor.npy\", mmap_mode='r')  # Memory-mapped mode\n",
    "labels = np.load(\"HOG_labels.npy\")\n",
    "\n",
    "# Verify unique labels\n",
    "unique_labels = np.unique(labels)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "# Encode labels\n",
    "print(\"Encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Split data\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5, algorithm='auto', weights='distance'),\n",
    "    \"SGD Classifier\": SGDClassifier(max_iter=1000, tol=1e-3),\n",
    "    \"Linear SVM\": SVC(kernel='linear', max_iter=2000),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Function to process data in batches\n",
    "def batch_predict(clf, X, batch_size=1000):\n",
    "    y_pred = []\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        batch = X[i:i+batch_size]\n",
    "        y_pred.extend(clf.predict(batch))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = []\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(f\"Model {name} trained in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Predict in batches\n",
    "    print(f\"Predicting with {name}...\")\n",
    "    y_pred = batch_predict(pipeline, X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Cross-validation score\n",
    "    print(f\"Performing cross-validation for {name}...\")\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})\")\n",
    "    \n",
    "    results.append((name, accuracy))\n",
    "\n",
    "print(\"\\nClassification complete.\")\n",
    "\n",
    "# Find the best classifier\n",
    "best_classifier, best_accuracy = max(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest classifier: {best_classifier} with accuracy {best_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d13e75-ece1-42e8-b2c6-6ac5a11be811",
   "metadata": {},
   "source": [
    "* Gaussian Naive Bayes accuracy: 0.54\n",
    "* K-Nearest Neighbors accuracy: 0.17\n",
    "* SGD Classifier accuracy: 0.55\n",
    "* Linear SVM accuracy: 0.61\n",
    "* MLP accuracy: 0.65\n",
    "* Random Forest accuracy: 0.48\n",
    "* Decision Tree accuracy: 0.40\n",
    "* Logistic Regression accuracy: 0.58\n",
    "* Gradient boost - 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471b7fc-b455-4765-8339-20570bf2dad4",
   "metadata": {},
   "source": [
    "\n",
    "* Output Explanation\n",
    "  \n",
    "Gaussian Naive Bayes (54% accuracy): This classifier assumes that features are independent. It performs moderately well because HOG features, which focus on local gradients, can somewhat meet this assumption, but it still lacks in modeling complex interactions.\n",
    "\n",
    "K-Nearest Neighbors (17% accuracy): KNN performs poorly in this setup, likely due to the high dimensionality of HOG features, which can lead to the \"curse of dimensionality,\" where distances become less meaningful.\n",
    "\n",
    "SGD Classifier (55% accuracy): The Stochastic Gradient Descent Classifier shows moderate performance. It benefits from fast training on large datasets but may struggle with non-linear data patterns.\n",
    "\n",
    "Linear SVM (61% accuracy): This classifier uses a linear decision boundary, which works well with HOG features capturing edges and gradients, resulting in good performance for this task.\n",
    "\n",
    "MLP (65% accuracy): The Multi-Layer Perceptron, a neural network, handles non-linear patterns effectively, benefiting from its ability to learn complex feature interactions, leading to relatively high accuracy.\n",
    "\n",
    "Random Forest (48% accuracy): Random Forest uses ensemble learning to model feature interactions but might struggle with the high dimensionality of HOG features, limiting its accuracy.\n",
    "\n",
    "Decision Tree (40% accuracy): Decision Trees have lower accuracy due to their tendency to overfit high-dimensional data and lack ensemble methods to balance this.\n",
    "\n",
    "Logistic Regression (58% accuracy): Logistic Regression performs better due to its ability to find linear decision boundaries that effectively separate the classes in the feature space.\n",
    "\n",
    "Gradient Boosting (69% accuracy): This method incrementally builds models that correct previous errors, capturing complex patterns and leading to the highest accuracy among individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dd383-8356-4c81-97a8-c856666b428a",
   "metadata": {},
   "source": [
    "### 3. Feature Extractor and Classifier - HOG + Colour Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8e560-140c-4720-8d80-88db2676999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import zscore\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Define paths to directories\n",
    "directories = [\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/green_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blue_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/red_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/noisy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blurry\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/hazy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/high_contrast\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/low_illumination\"\n",
    "]\n",
    "\n",
    "def extract_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_image = cv2.resize(gray_image, (128, 128))\n",
    "    \n",
    "    # Extract HOG features\n",
    "    hog_features = hog(resized_image, pixels_per_cell=(16, 16),\n",
    "                       cells_per_block=(2, 2), feature_vector=True)\n",
    "    \n",
    "   \n",
    "    color_features = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    color_features = color_features.flatten()\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = np.concatenate((hog_features, color_features))\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "def process_image(args):\n",
    "    image_path, label = args\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            features = extract_features(image)\n",
    "            return features, label, image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def process_directory(directory, label):\n",
    "    image_paths = [os.path.join(directory, filename) for filename in os.listdir(directory)]\n",
    "    args = [(image_path, label) for image_path in image_paths]\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(process_image, args), total=len(args), desc=f\"Processing {os.path.basename(directory)}\"))\n",
    "    \n",
    "    return [result for result in results if result is not None]\n",
    "\n",
    "def augment_data(features, labels):\n",
    "    augmented_features = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for feature, label in zip(features, labels):\n",
    "        augmented_features.append(feature)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, 0.1, feature.shape)\n",
    "        augmented_features.append(feature + noise)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Scale features\n",
    "        augmented_features.append(feature * 1.1)\n",
    "        augmented_labels.append(label)\n",
    "    \n",
    "    return np.array(augmented_features), np.array(augmented_labels)\n",
    "\n",
    "# Prepare dataset\n",
    "all_results = []\n",
    "for idx, directory in enumerate(directories):\n",
    "    all_results.extend(process_directory(directory, idx))\n",
    "\n",
    "# Separate features, labels, and image paths\n",
    "features, labels, image_paths = zip(*all_results)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Normalize features\n",
    "features = zscore(features)\n",
    "\n",
    "# Augment data\n",
    "features, labels = augment_data(features, labels)\n",
    "\n",
    "# Save features and labels\n",
    "np.save('SparseHOGFeatures.npy', features)\n",
    "np.save('SparseHOGLabels.npy', labels)\n",
    "\n",
    "print(f\"Saved {len(features)} feature vectors and labels\")\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers and their parameter grids for hyperparameter tuning\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": (GaussianNB(), {}),\n",
    "    \"K-Nearest Neighbors\": (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7, 9]}),\n",
    "    \"Logistic Regression\": (LogisticRegression(solver='saga', max_iter=5000), {'C': [0.1, 1, 10, 100]}),\n",
    "    \"Support Vector Machine\": (SVC(), {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}),\n",
    "    \"Multilayer Perceptron\": (MLPClassifier(max_iter=1000), {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]}),\n",
    "    \"Random Forest\": (RandomForestClassifier(), {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {'max_depth': [None, 10, 20, 30]})\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers with hyperparameter tuning\n",
    "best_classifier = None\n",
    "best_accuracy = 0\n",
    "results = []\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for name, (clf, param_grid) in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(clf, param_grid, n_iter=10, cv=5, n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_clf = random_search.best_estimator_\n",
    "    end_time = time.time()\n",
    "    print(f\"Training of {name} completed with best parameters: {random_search.best_params_} in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"Evaluating {name}...\")\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append((name, accuracy))\n",
    "    print(f\"{name} accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy and accuracy <= 0.99:\n",
    "        best_accuracy = accuracy\n",
    "        best_classifier = name\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"Classification report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    print(f\"Displaying confusion matrix for {name}...\")\n",
    "    ConfusionMatrixDisplay.from_estimator(best_clf, X_test, y_test)\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Print the best classifier\n",
    "print(f\"\\nThe best classifier is {best_classifier} with an accuracy of {best_accuracy:.2f}\")\n",
    "\n",
    "# Visualize classifier performance\n",
    "print(\"Visualizing classifier performance...\")\n",
    "classifier_names = [result[0] for result in results]\n",
    "accuracies = [result[1] for result in results]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(classifier_names, accuracies, color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier Performance')\n",
    "plt.xlim(0.8, 1)  # Adjusted to show only the relevant range\n",
    "plt.axvline(0.80, color='red', linestyle='--', label='80% Accuracy Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize 5 random images with their HOG features\n",
    "random_indices = random.sample(range(len(image_paths)), 5)\n",
    "for idx in random_indices:\n",
    "    image_path = image_paths[idx]\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_image = cv2.resize(gray_image, (128, 128))\n",
    "    hog_features, hog_image = hog(resized_image, pixels_per_cell=(16, 16),\n",
    "                                  cells_per_block=(2, 2), visualize=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(hog_image, cmap='gray')\n",
    "    plt.title(\"HOG Features\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922038b6-bc5c-40ad-92a4-56ad225edc3e",
   "metadata": {},
   "source": [
    "* Gaussian Naive  Bayes     -    87%\n",
    "* KNN                       -    73%\n",
    "* SVM                       -    89%\n",
    "* Decision Tree             -    89%\n",
    "* Naive Bayes               -    82%\n",
    "* Logistic Regression       -    85%\n",
    "* MLP                       -    95%\n",
    "* Random Forest Classifier  -    97%\n",
    "* Gradient Boost            -    99%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caed07-c0f3-4cca-ac58-8fb579e11f48",
   "metadata": {},
   "source": [
    "### Output Explanation\n",
    "\n",
    "1. **Gaussian Naive Bayes (87% accuracy):** This classifier assumes independence among features and is effective for this task due to its simplicity and ability to handle noise in the data.\n",
    "\n",
    "2. **K-Nearest Neighbors (73% accuracy):** KNN relies on distances between data points, which can be less effective with high-dimensional data like HOG features, leading to lower accuracy.\n",
    "\n",
    "3. **Support Vector Machine (89% accuracy):** SVM is effective in finding a hyperplane that separates different classes well, leading to high accuracy. It can handle high-dimensional data efficiently.\n",
    "\n",
    "4. **Decision Tree (89% accuracy):** Decision Trees perform well by modeling complex decision boundaries, which is beneficial for this dataset with diverse image degradations.\n",
    "\n",
    "5. **Naive Bayes (82% accuracy):** Similar to Gaussian Naive Bayes, this classifier is robust to noise and performs well with the independent feature assumption.\n",
    "\n",
    "6. **Logistic Regression (85% accuracy):** Logistic Regression provides good performance by finding a linear decision boundary in the feature space, capturing the essential patterns in the data.\n",
    "\n",
    "7. **MLP (95% accuracy):** The Multi-Layer Perceptron, a type of neural network, excels in learning complex, non-linear relationships within the data, resulting in high accuracy.\n",
    "\n",
    "8. **Random Forest Classifier (97% accuracy):** Random Forests use ensemble learning, combining multiple decision trees to improve accuracy by reducing overfitting and capturing complex patterns.\n",
    "\n",
    "9. **Gradient Boosting (99% accuracy):** Gradient Boosting incrementally builds models to correct previous errors, effectively capturing subtle patterns in the data, leading to the highest accuracy.\n",
    "\n",
    "The variations in accuracy are due to each classifier's ability to model the complex patterns and feature interactions present in the image data, with ensemble methods like Random Forest and Gradient Boosting performing exceptionally well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795e5a2-40a2-48d7-8671-f442a3fc695b",
   "metadata": {},
   "source": [
    "### 4. Feature Extraction and Classifier Using Sparse+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762d564-6d0d-4d11-b51d-63ac5e909a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_extract_features(degraded_folders):\n",
    "    features = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "\n",
    "    for folder in degraded_folders:\n",
    "        label = os.path.basename(folder)\n",
    "        print(f\"Processing folder: {label}\")\n",
    "        for image_file in os.listdir(folder):\n",
    "            image_path = os.path.join(folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            print(f\"Processing image: {image_file}\")\n",
    "            if image is None:\n",
    "                print(f\"Failed to load image: {image_file}\")\n",
    "                continue\n",
    "            \n",
    "           \n",
    "            hist_features = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n",
    "            features.append(hist_features)\n",
    "            labels.append(label)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Apply SparsePCA\n",
    "    spca = SparsePCA(n_components=3, random_state=42)\n",
    "    features_spca = spca.fit_transform(features_scaled)\n",
    "    \n",
    "    return features_spca, labels, image_paths\n",
    "\n",
    "def visualize_features(image_paths, features):\n",
    "    indices = np.random.choice(len(image_paths), 10, replace=False)\n",
    "    for i in indices:\n",
    "        image_path = image_paths[i]\n",
    "        feature = features[i]\n",
    "        print(f\"Image: {os.path.basename(image_path)}, Features: {feature}\")\n",
    "        plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Features: {feature}\")\n",
    "        plt.show()\n",
    "\n",
    "degraded_folders = [\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/green_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blue_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/red_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/noisy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blurry\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/hazy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/high_contrast\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/low_illumination\"\n",
    "]\n",
    "\n",
    "# Load features and labels\n",
    "features, labels, image_paths = load_and_extract_features(degraded_folders)\n",
    "\n",
    "# Visualize 10 random images with their features\n",
    "visualize_features(image_paths, features)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"SVM\": SVC(kernel='linear', C=1.0, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boost\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "best_accuracy = 0\n",
    "best_classifier_name = \"\"\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if name == \"Multinomial Naive Bayes\":\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report for {name}:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_classifier_name = name\n",
    "\n",
    "print(f\"Best Classifier: {best_classifier_name} with Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3745c1-1866-49b7-b535-43bb8f6147f4",
   "metadata": {},
   "source": [
    "### Sparse+PCA method\n",
    "\n",
    "* Gaussian Naive Bayes Accuracy: 0.3848314606741573\n",
    "\n",
    "* K-Nearest Neighbors Accuracy: 0.7591292134831461\n",
    "\n",
    "* Logistic Regression Accuracy: 0.5639044943820225\n",
    "\n",
    "* SVM Accuracy: 0.6144662921348315\n",
    "\n",
    "* MLP Accuracy: 0.7191011235955056\n",
    "\n",
    "* Random Forest Accuracy: 0.7851123595505618\n",
    "\n",
    "* Gradient Boost Accuracy: 0.7612359550561798\n",
    "\n",
    "* Decision Tree Accuracy: 0.7415730337078652\n",
    "\n",
    "* Multinomial Naive Bayes Accuracy: 0.15660112359550563"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0b526-95af-4e84-b2f4-744d615aa1e9",
   "metadata": {},
   "source": [
    "### Output Explanation\n",
    "\n",
    "1. **Gaussian Naive Bayes (38% accuracy):** This classifier assumes feature independence and may struggle due to the complex dependencies in image data, leading to lower accuracy.\n",
    "\n",
    "2. **K-Nearest Neighbors (76% accuracy):** KNN benefits from the simplified feature space after SparsePCA, allowing it to effectively classify images based on feature similarity.\n",
    "\n",
    "3. **Logistic Regression (56% accuracy):** Logistic Regression attempts to find linear decision boundaries, but the reduced feature space might not capture all complex patterns, resulting in moderate accuracy.\n",
    "\n",
    "4. **SVM (61% accuracy):** SVM's linear kernel helps it find separating hyperplanes, but the reduced complexity of the data makes it less effective compared to non-linear kernels.\n",
    "\n",
    "5. **MLP (72% accuracy):** The Multi-Layer Perceptron can learn complex, non-linear relationships even in the reduced feature space, resulting in relatively high accuracy.\n",
    "\n",
    "6. **Random Forest (79% accuracy):** Random Forest's ensemble of decision trees captures patterns effectively, even with reduced dimensions, providing high accuracy.\n",
    "\n",
    "7. **Gradient Boost (76% accuracy):** Gradient Boosting incrementally corrects errors and performs well in capturing patterns, leading to high accuracy despite feature reduction.\n",
    "\n",
    "8. **Decision Tree (74% accuracy):** Decision Trees adapt well to the simplified feature space, finding decision boundaries that separate classes effectively.\n",
    "\n",
    "9. **Multinomial Naive Bayes:** This classifier requires non-negative integer features (like word counts), making it unsuitable for this task, resulting in incompatibility with the feature scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c265683-0809-40ab-8baa-25ebff7d7240",
   "metadata": {},
   "source": [
    "### 5. Feature Extraction and Classifier Using Sparse+HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f66664-a80f-4a03-bbc1-64f55b1653b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Process an image and extract features.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {os.path.basename(image_path)}\")\n",
    "        return None\n",
    "\n",
    "    # Convert to grayscale for HOG\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Extract HOG features\n",
    "    hog_features = hog(gray_image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False, feature_vector=True)\n",
    "\n",
    "    # Extract color histogram features\n",
    "    hist_features = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n",
    "\n",
    "    # Ensure hist_features and hog_features have the same length\n",
    "    if len(hog_features) < 3780:\n",
    "        hog_features = np.pad(hog_features, (0, 3780 - len(hog_features)), 'constant')\n",
    "    elif len(hog_features) > 3780:\n",
    "        hog_features = hog_features[:3780]\n",
    "\n",
    "    combined_features = np.hstack((hist_features, hog_features))\n",
    "    return combined_features\n",
    "\n",
    "def extract_sparse_hog_features(degraded_folders, batch_size=100):\n",
    "    features = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "\n",
    "    for folder in degraded_folders:\n",
    "        label = os.path.basename(folder)\n",
    "        print(f\"Processing folder: {label}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        image_files = [os.path.join(folder, image_file) for image_file in os.listdir(folder)]\n",
    "        \n",
    "        for batch_start in range(0, len(image_files), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(image_files))\n",
    "            batch_files = image_files[batch_start: batch_end]\n",
    "\n",
    "            # Use a thread pool to process images concurrently\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = {executor.submit(process_image, image_file): image_file for image_file in batch_files}\n",
    "\n",
    "                for idx, future in enumerate(as_completed(futures)):\n",
    "                    image_file = futures[future]\n",
    "                    result = future.result()\n",
    "\n",
    "                    if result is not None:\n",
    "                        features.append(result)\n",
    "                        labels.append(label)\n",
    "                        image_paths.append(image_file)\n",
    "\n",
    "                    # Print progress\n",
    "                    if (batch_start + idx + 1) % 10 == 0:\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        print(f\"Processed {batch_start + idx + 1} images in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply SparsePCA\n",
    "    spca = SparsePCA(n_components=3, random_state=42)\n",
    "    features_spca = spca.fit_transform(features_scaled)\n",
    "\n",
    "    # Save features and labels\n",
    "    np.save('SparseHOGextractedcolor.npy', features_spca)\n",
    "    np.save('SparseHOG_labels.npy', labels)\n",
    "\n",
    "    return features_spca, labels, image_paths\n",
    "\n",
    "def visualize_features(image_paths, features):\n",
    "    indices = np.random.choice(len(image_paths), 10, replace=False)\n",
    "    for i in indices:\n",
    "        image_path = image_paths[i]\n",
    "        feature = features[i]\n",
    "        print(f\"Image: {os.path.basename(image_path)}, Features: {feature}\")\n",
    "        plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Features: {feature}\")\n",
    "        plt.show()\n",
    "\n",
    "# Paths to the folders containing degraded images\n",
    "degraded_folders = [\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/green_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blue_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/red_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/noisy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blurry\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/hazy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/high_contrast\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/low_illumination\"\n",
    "]\n",
    "\n",
    "# Load features and labels\n",
    "features, labels, image_paths = extract_sparse_hog_features(degraded_folders)\n",
    "\n",
    "# Visualize 10 random images with their features\n",
    "visualize_features(image_paths, features)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"SVM\": SVC(kernel='linear', C=1.0, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boost\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "best_accuracy = 0\n",
    "best_classifier_name = \"\"\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Classification Report for {name}:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    if best_accuracy < accuracy < 1.0:  # Ensure it's not perfect 100%\n",
    "        best_accuracy = accuracy\n",
    "        best_classifier_name = name\n",
    "\n",
    "print(f\"Best Classifier: {best_classifier_name} with Accuracy: {best_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d613352-acbb-4b6d-b242-e3abf3a1135e",
   "metadata": {},
   "source": [
    "### Sparse+HOG\n",
    "\n",
    "* Gaussian Naive Bayes Accuracy: 0.32\n",
    "* K-Nearest Neighbors Accuracy: 0.35\n",
    "* Logistic Regression Accuracy: 0.34\n",
    "* SVM Accuracy: 0.35\n",
    "* MLP Accuracy: 0.38\n",
    "* Random Forest Accuracy: 0.35\n",
    "* Gradient Boost Accuracy: 0.38\n",
    "* Decision Tree Accuracy: 0.35\n",
    "* Bayes accuracy - 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf00065-ef6e-49e3-b7ad-e36e3d0aae94",
   "metadata": {},
   "source": [
    "\n",
    "### Output Explanation\n",
    "\n",
    "1. **Gaussian Naive Bayes (32% accuracy):** The low accuracy reflects Gaussian Naive Bayes' assumption of feature independence, which is not suitable for the combined HOG and Sparse.\n",
    "\n",
    "2. **K-Nearest Neighbors (35% accuracy):** KNN struggles due to the limited dimensionality reduction, which might not capture sufficient information to differentiate classes effectively.\n",
    "\n",
    "3. **Logistic Regression (34% accuracy):** Logistic Regression finds linear decision boundaries, but the reduced feature space might not fully capture complex class separations.\n",
    "\n",
    "4. **SVM (35% accuracy):** The linear kernel of SVM might not be complex enough to differentiate between classes in the reduced feature space.\n",
    "\n",
    "5. **MLP (38% accuracy):** The Multi-Layer Perceptron can learn non-linear patterns and has slightly better performance, but the reduced complexity limits its potential.\n",
    "\n",
    "6. **Random Forest (35% accuracy):** While Random Forest is typically robust, the feature reduction limits its ability to capture diverse patterns across many trees.\n",
    "\n",
    "7. **Gradient Boost (38% accuracy):** Similar to Random Forest, Gradient Boosting performs better than some models but is limited by the reduced complexity of the features.\n",
    "\n",
    "8. **Decision Tree (35% accuracy):** Decision Trees perform similarly to Random Forest, as they might overfit to the reduced, potentially incomplete feature set.\n",
    "\n",
    "9. **Bayes accuracy (30% accuracy):** This score suggests challenges in capturing dependencies between features in the reduced dataset, leading to low performance.\n",
    "\n",
    "The relatively low accuracy across classifiers is due to the feature extraction and reduction process that might not preserve enough distinguishing information for effective classification across complex image degradations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f6d2f-f6de-4061-a6c5-6a13300d8e68",
   "metadata": {},
   "source": [
    "### 6. Feature Extractor Using Hog+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fde19a-1bd8-492d-ac5f-ae51c1df9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define directories\n",
    "directories = [\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/green_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blue_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/red_tint\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/noisy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/blurry\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/hazy\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/high_contrast\",\n",
    "    \"C:/Users/abhis/Downloads/degraded_images/low_illumination\"\n",
    "]\n",
    "\n",
    "# Function to extract PCA color features\n",
    "def extract_pca_color_features(image, max_components=10):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pixels = image_rgb.reshape(-1, 3).astype(float) / 255.0\n",
    "    n_samples, n_features = pixels.shape\n",
    "    n_components = min(max_components, n_samples, n_features)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(pixels)\n",
    "    components = np.pad(pca.components_.flatten(), (0, max_components * 3 - len(pca.components_.flatten())))\n",
    "    explained_variance_ratio = np.pad(pca.explained_variance_ratio_, (0, max_components - len(pca.explained_variance_ratio_)))\n",
    "    return np.concatenate((components, explained_variance_ratio))\n",
    "\n",
    "# Function to extract HOG color features\n",
    "def extract_hog_color_features(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    image_resized = cv2.resize(image, (128, 128))\n",
    "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    hog_features = []\n",
    "    for channel in range(3):  # Extract HOG for each color channel\n",
    "        channel_hog = hog(\n",
    "            image_rgb[:,:,channel], \n",
    "            pixels_per_cell=pixels_per_cell,\n",
    "            cells_per_block=cells_per_block,\n",
    "            orientations=orientations,\n",
    "            visualize=False\n",
    "        )\n",
    "        hog_features.extend(channel_hog)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "# List to hold all extracted features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "processed_images = 0\n",
    "\n",
    "for folder in directories:\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        try:\n",
    "            # Read the image\n",
    "            image = cv2.imread(filepath)\n",
    "            if image is not None:\n",
    "                # Extract PCA and HOG features and concatenate them\n",
    "                pca_features = extract_pca_color_features(image)\n",
    "                hog_features = extract_hog_color_features(image)\n",
    "                combined_features = np.concatenate((pca_features, hog_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(os.path.basename(folder))\n",
    "                processed_images += 1\n",
    "                print(f\"Processed {filename} successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to read {filename}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        # Print progress update\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if processed_images > 0 and elapsed_time > 0:\n",
    "            print(f\"Current processing speed: {processed_images / elapsed_time:.2f} images/sec\")\n",
    "        else:\n",
    "            print(\"Processing speed cannot be calculated yet.\")\n",
    "\n",
    "    print(f\"Finished processing folder: {folder}\")\n",
    "\n",
    "# Save the extracted features and labels\n",
    "np.save(\"Combined_PCA_HOG_color_features.npy\", np.array(all_features))\n",
    "np.save(\"Combined_PCA_HOG_color_labels.npy\", np.array(all_labels))\n",
    "\n",
    "print(\"Feature extraction complete.\")\n",
    "\n",
    "# Print 10 random images with their combined PCA and HOG features\n",
    "sample_indices = random.sample(range(len(all_features)), min(10, len(all_features)))\n",
    "for idx in sample_indices:\n",
    "    print(f\"Image: {all_labels[idx]}\")\n",
    "    print(f\"Combined PCA and HOG Features: {all_features[idx]}\")\n",
    "\n",
    "    # Visualization (optional)\n",
    "    img_folder = [d for d in directories if os.path.basename(d) == all_labels[idx]][0]\n",
    "    img_files = os.listdir(img_folder)\n",
    "    if img_files:\n",
    "        img_path = os.path.join(img_folder, img_files[0])\n",
    "        plt.imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Combined Features for {os.path.basename(img_path)}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images found in {img_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff583470-9cce-45a4-a826-1406b4e9d76b",
   "metadata": {},
   "source": [
    "### Classifier for  Hog+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1bc25-028a-4d2c-b142-9dacc40ef0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load features and labels\n",
    "features_path = \"D:/IIT/Combined_PCA_HOG_color_features.npy\"\n",
    "labels_path = \"D:/IIT/Combined_PCA_HOG_color_labels.npy\"\n",
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "# Check the unique labels to ensure they are correct\n",
    "print(\"Unique labels:\", np.unique(labels))\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=500)  # Adjust the number of components based on memory limitations and performance\n",
    "features_reduced = pca.fit_transform(features)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_reduced, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Define classifiers with hyperparameter grids\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": (GaussianNB(), {}),\n",
    "    \"K-Nearest Neighbors\": (KNeighborsClassifier(), {'classifier__n_neighbors': [3, 5, 7, 9]}),\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=1000), {'classifier__C': [0.1, 1, 10]}),\n",
    "    \"SVM\": (SVC(), {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['rbf', 'linear']}),\n",
    "    \"MLP\": (MLPClassifier(max_iter=1000), {'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)]}),\n",
    "    \"Random Forest\": (RandomForestClassifier(), {'classifier__n_estimators': [50, 100, 200]}),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(), {'classifier__n_estimators': [50, 100, 200], 'classifier__learning_rate': [0.01, 0.1]}),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {'classifier__max_depth': [5, 10, None]})\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling and classifier\n",
    "def create_pipeline(clf):\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "best_classifiers = {}\n",
    "\n",
    "for name, (clf, param_grid) in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipeline = create_pipeline(clf)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    best_classifiers[name] = (grid_search.best_estimator_, accuracy)\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, clf) for name, (clf, _) in best_classifiers.items()],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "\n",
    "print(\"\\nVoting Classifier Results:\")\n",
    "print(f\"Accuracy: {voting_accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred_voting))\n",
    "\n",
    "# Find the best individual classifier\n",
    "best_classifier = max(best_classifiers.items(), key=lambda x: x[1][1])\n",
    "print(f\"\\nBest individual classifier: {best_classifier[0]} with accuracy {best_classifier[1][1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2fcc4-31e4-4769-980f-834809d8a95d",
   "metadata": {},
   "source": [
    "### HOG + PCA method\n",
    "\n",
    "* Gaussian Naive Bayes accuracy: 0.42\n",
    "* K-Nearest Neighbors accuracy: 0.13\n",
    "* Logistic Regression accuracy: 0.55\n",
    "* SVM accuracy: 0.51\n",
    "* MLP accuracy: 0.58\n",
    "* Random Forest accuracy: 0.37\n",
    "* Gradient Boosting - 0.67\n",
    "* Decision tree - 0. 56\n",
    "* Bayes - 0.32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd93779-e0de-482b-b218-d90b9261bc6a",
   "metadata": {},
   "source": [
    "### Output Explanation\n",
    "\n",
    "1. **Gaussian Naive Bayes (42% accuracy):** This classifier performs moderately well because it assumes feature independence, which can be a reasonable approximation when using features that capture distinct aspects of the data.\n",
    "\n",
    "2. **K-Nearest Neighbors (13% accuracy):** The low accuracy is due to KNN's sensitivity to feature scaling and density of data points in high-dimensional space, making it less effective with features.\n",
    "\n",
    "3. **Logistic Regression (55% accuracy):** Logistic Regression performs decently as it models linear decision boundaries\n",
    "\n",
    "4. **SVM (51% accuracy):** Support Vector Machine uses linear or RBF kernels to find a hyperplane that separates classes, and while useful.\n",
    "\n",
    "5. **MLP (58% accuracy):** The Multi-Layer Perceptron, a type of neural network, shows good performance as it can learn non-linear patterns in the data, benefiting from both PCA and HOG features.\n",
    "\n",
    "6. **Random Forest (37% accuracy):** The lower accuracy is likely due to Random Forest's dependence on multiple decision trees, which may overfit to noise in the reduced feature set.\n",
    "\n",
    "7. **Gradient Boosting (67% accuracy):** This classifier achieves the highest accuracy because it iteratively builds an ensemble of weak learners that effectively capture complex patterns, leveraging the PCA and HOG features.\n",
    "\n",
    "8. **Decision Tree (56% accuracy):** Decision Trees can model non-linear boundaries and work well with reduced feature sets, but can also overfit without careful pruning.\n",
    "\n",
    "9. **Bayes (32% accuracy):** This reflects a simpler model's limitations in capturing complex patterns inherent in the dataset with reduced feature representation.\n",
    "\n",
    "The use of PCA and HOG features together enhances the classifiers' ability to capture color and texture information, but the effectiveness varies across models depending on how well they can leverage this combined feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5911fc-2dfb-41ed-8a4c-2e99ce671f49",
   "metadata": {},
   "source": [
    "# 7. Feature Extraction using Colour Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b3c7f-6486-4ad0-ada6-96c469df5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def load_features_and_labels(degraded_folders):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for folder in degraded_folders:\n",
    "        label = os.path.basename(folder)\n",
    "        for image_file in os.listdir(folder):\n",
    "            image_path = os.path.join(folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            hist_features = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n",
    "            features.append(hist_features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "degraded_folders = [\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\green_tint\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\blue_tint\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\red_tint\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\noisy\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\blurry\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\hazy\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\high_contrast\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\low_illumination\"\n",
    "]\n",
    "\n",
    "features, labels = load_features_and_labels(degraded_folders)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571be785-9fe2-4160-9e79-93415bcf649b",
   "metadata": {},
   "source": [
    "### Classifier using Colour Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc345f1a-cb4d-4256-b342-14c2ef94bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def load_features_and_labels(degraded_folders):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for folder in degraded_folders:\n",
    "        label = os.path.basename(folder)\n",
    "        for image_file in os.listdir(folder):\n",
    "            image_path = os.path.join(folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            hist_features = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n",
    "            features.append(hist_features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "# Define folders\n",
    "degraded_folders = [\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\green_tint\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\blue_tint\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\red_tint\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\noisy\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\blurry\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\hazy\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\high_contrast\",\n",
    "    \"C:\\\\Users\\\\abhis\\\\Downloads\\\\degraded_images\\\\low_illumination\"\n",
    "]\n",
    "\n",
    "# Load features and labels\n",
    "features, labels = load_features_and_labels(degraded_folders)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of classifiers\n",
    "classifiers = {\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"Bernoulli Naive Bayes\": BernoulliNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test, y_pred, target_names=label_encoder.classes_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e124de-f4f9-4649-b0d1-0a97d81789f8",
   "metadata": {},
   "source": [
    "* Gaussian Naive  Bayes - 90.38%\n",
    "* KNN - 90.24%\n",
    "* SVM - 93.11%\n",
    "* Decision Tree - 92.69%\n",
    "* Naive Bayes - 89.46%\n",
    "* Logistic Regression  - 94.03%\n",
    "* MLP - 91.92%\n",
    "* Random Forest Classifier - 97.40%\n",
    "* Gradient Boost - 96.83%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85301ceb-d437-4803-a37d-bbca305cbd64",
   "metadata": {},
   "source": [
    "### Output Explanation\n",
    "\n",
    "1. **Gaussian Naive Bayes (90.38% accuracy):** This classifier performs well with the color histogram data as it assumes feature independence, effectively using the distributions of pixel values.\n",
    "\n",
    "2. **K-Nearest Neighbors (90.24% accuracy):** KNN works well by finding similarities in color distributions between images, which is why it achieves relatively high accuracy with this type of feature.\n",
    "\n",
    "3. **Support Vector Machine (SVM) (93.11% accuracy):** The SVM is effective at finding the optimal hyperplane for classification, making it a robust choice for the given histogram features.\n",
    "\n",
    "4. **Decision Tree (92.69% accuracy):** The Decision Tree can capture complex interactions between color features, allowing it to perform well in classifying the images based on their histograms.\n",
    "\n",
    "5. **Multinomial Naive Bayes (89.46% accuracy):** This classifier is slightly less effective with continuous data but still achieves good performance due to the feature representation's discreteness.\n",
    "\n",
    "6. **Logistic Regression (94.03% accuracy):** Logistic Regression models linear relationships between features and labels well, resulting in high accuracy with the color histogram data.\n",
    "\n",
    "7. **MLP Classifier (91.92% accuracy):** The MLP's ability to learn non-linear patterns helps it to effectively classify the images based on the histograms.\n",
    "\n",
    "8. **Random Forest Classifier (97.40% accuracy):** Random Forest achieves the highest accuracy due to its ensemble approach, combining multiple decision trees to capture a wide range of patterns in the data.\n",
    "\n",
    "9. **Gradient Boosting (96.83% accuracy):** This model is highly effective due to its iterative approach to improving predictions, capturing complex patterns within the histogram features.\n",
    "\n",
    "Overall, the color histogram effectively represents the distribution of pixel intensities, which these classifiers can leverage to distinguish between different types of image degradations. Ensemble methods like Random Forest and Gradient Boosting are particularly effective because they combine the strengths of multiple models to achieve high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb84f08-102f-4387-89e6-373e4473fed9",
   "metadata": {},
   "source": [
    "### 8. Feature Extractor using Different Color extraction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ec7ad-ef46-4005-9e94-31cb2ad20f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define feature extraction methods\n",
    "def extract_dominant_color_descriptor(image, num_colors=5):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=num_colors, random_state=42)\n",
    "    kmeans.fit(pixels)\n",
    "    dominant_colors = kmeans.cluster_centers_\n",
    "    return dominant_colors.flatten()\n",
    "\n",
    "def extract_color_coherence_vector(image, num_bins=64, threshold=0.1):\n",
    "    h, w, c = image.shape\n",
    "    bins = np.linspace(0, 256, num_bins + 1, endpoint=True)\n",
    "    quantized = np.digitize(image, bins) - 1\n",
    "    ccv = np.zeros((num_bins, c, 2), dtype=int)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            for channel in range(c):\n",
    "                region = quantized[max(0, y - 1):min(h, y + 2), max(0, x - 1):min(w, x + 2), channel]\n",
    "                coherence = np.sum(region == quantized[y, x, channel]) >= (region.size * threshold)\n",
    "                ccv[quantized[y, x, channel], channel, coherence] += 1\n",
    "    return ccv.flatten()\n",
    "\n",
    "def extract_color_layout_descriptor(image, grid_size=(8, 8)):\n",
    "    h, w, _ = image.shape\n",
    "    grid_h, grid_w = h // grid_size[0], w // grid_size[1]\n",
    "    cld = []\n",
    "    for i in range(0, h, grid_h):\n",
    "        for j in range(0, w, grid_w):\n",
    "            grid = image[i:i + grid_h, j:j + grid_w]\n",
    "            avg_color = np.mean(grid, axis=(0, 1))\n",
    "            cld.extend(avg_color)\n",
    "    return np.array(cld)\n",
    "\n",
    "def extract_hsv_histogram(image, bins=(16, 16, 16)):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "def extract_color_name_descriptor(image):\n",
    "    COLOR_NAMES = [\n",
    "        (\"red\", (255, 0, 0)),\n",
    "        (\"green\", (0, 255, 0)),\n",
    "        (\"blue\", (0, 0, 255)),\n",
    "        (\"yellow\", (255, 255, 0)),\n",
    "        (\"magenta\", (255, 0, 255)),\n",
    "        (\"cyan\", (0, 255, 255)),\n",
    "        (\"black\", (0, 0, 0)),\n",
    "        (\"white\", (255, 255, 255)),\n",
    "        (\"gray\", (128, 128, 128)),\n",
    "        (\"orange\", (255, 165, 0)),\n",
    "        (\"brown\", (165, 42, 42)),\n",
    "        (\"pink\", (255, 192, 203))\n",
    "    ]\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=len(COLOR_NAMES), random_state=42)\n",
    "    kmeans.fit(pixels)\n",
    "    dominant_colors = kmeans.cluster_centers_\n",
    "    color_name_descriptor = np.zeros(len(COLOR_NAMES))\n",
    "    for color in dominant_colors:\n",
    "        distances = [np.linalg.norm(color - np.array(rgb)) for name, rgb in COLOR_NAMES]\n",
    "        closest_color_index = np.argmin(distances)\n",
    "        color_name_descriptor[closest_color_index] += 1\n",
    "    return color_name_descriptor\n",
    "\n",
    "def extract_opponency_color_features(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    r, g, b = cv2.split(image)\n",
    "    rg = r - g\n",
    "    yb = 0.5 * (r + g) - b\n",
    "    opponent_features = np.concatenate((rg.flatten(), yb.flatten()))\n",
    "    return opponent_features\n",
    "\n",
    "# Load images from folder\n",
    "def load_images(folder_path, num_images=50):\n",
    "    print(f\"Loading images from {folder_path}\")\n",
    "    image_paths = glob(os.path.join(folder_path, '*'))\n",
    "    selected_images = random.sample(image_paths, min(num_images, len(image_paths)))\n",
    "    images = []\n",
    "    for image_path in selected_images:\n",
    "        print(f\"Processing image: {os.path.basename(image_path)}\")\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "        else:\n",
    "            print(f\"Failed to load image: {os.path.basename(image_path)}\")\n",
    "    return images\n",
    "\n",
    "# Process each folder and extract features\n",
    "def process_folder(folder_path, feature_extraction_function):\n",
    "    images = load_images(folder_path)\n",
    "    features = []\n",
    "    for idx, image in enumerate(images):\n",
    "        feature = feature_extraction_function(image)\n",
    "        features.append(feature)\n",
    "        print(f\"Extracted features from image {idx + 1}/{len(images)}\")\n",
    "    return features\n",
    "\n",
    "# Pad or truncate features to the same length\n",
    "def pad_features(features, max_length):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) < max_length:\n",
    "            feature = np.pad(feature, (0, max_length - len(feature)), 'constant')\n",
    "        else:\n",
    "            feature = feature[:max_length]\n",
    "        padded_features.append(feature)\n",
    "    return np.array(padded_features)\n",
    "\n",
    "# Classifiers to evaluate\n",
    "classifiers = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVC\": SVC(kernel='linear'),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Define feature extraction methods\n",
    "feature_methods = {\n",
    "    \"Dominant Color Descriptor\": extract_dominant_color_descriptor,\n",
    "    \"Color Coherence Vector\": extract_color_coherence_vector,\n",
    "    \"Color Layout Descriptor\": extract_color_layout_descriptor,\n",
    "    \"HSV Histogram\": extract_hsv_histogram,\n",
    "    \"Color Name Descriptor\": extract_color_name_descriptor,\n",
    "    \"Opponency Color Features\": extract_opponency_color_features\n",
    "}\n",
    "\n",
    "# Define degradation folders\n",
    "folders = [\n",
    "    \"green_tint\",\n",
    "    \"blue_tint\",\n",
    "    \"red_tint\",\n",
    "    \"noisy\",\n",
    "    \"blurry\",\n",
    "    \"hazy\",\n",
    "    \"high_contrast\",\n",
    "    \"low_illumination\"\n",
    "]\n",
    "\n",
    "# Load data, extract features, and classify\n",
    "def main():\n",
    "    base_path = \"C:/Users/abhis/Downloads/degraded_images/\"\n",
    "    best_results = {method_name: (None, 0) for method_name in feature_methods.keys()}\n",
    "    accuracy_results = {method_name: {} for method_name in feature_methods.keys()}\n",
    "\n",
    "    for method_name, feature_method in feature_methods.items():\n",
    "        print(f\"\\nExtracting features using {method_name}...\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            print(f\"  Processing folder: {folder_path}\")\n",
    "            \n",
    "            # Extract features\n",
    "            features = process_folder(folder_path, feature_method)\n",
    "            max_length = max(len(f) for f in features)\n",
    "            features = pad_features(features, max_length)\n",
    "            labels = np.full(len(features), folder)\n",
    "            \n",
    "            all_features.extend(features)\n",
    "            all_labels.extend(labels)\n",
    "        \n",
    "        all_features = np.array(all_features)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Split data\n",
    "        print(\"  Splitting data into training and testing sets...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Evaluate each classifier\n",
    "        for clf_name, classifier in classifiers.items():\n",
    "            print(f\"    Training and evaluating {clf_name}...\")\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate classifier\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"    Accuracy using {method_name} with {clf_name}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "            # Keep track of the best classifier for this feature extraction method\n",
    "            if accuracy > best_results[method_name][1]:\n",
    "                best_results[method_name] = (clf_name, accuracy)\n",
    "\n",
    "            accuracy_results[method_name][clf_name] = accuracy\n",
    "\n",
    "    # Output best classifiers for each method\n",
    "    for method_name, (best_clf, accuracy) in best_results.items():\n",
    "        print(f\"\\nThe best classifier for {method_name} is {best_clf} with accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a42e5-98ba-43d5-ae8c-48362483724d",
   "metadata": {},
   "source": [
    "### 1. Dominant Color Descriptor\n",
    "\n",
    "* Accuracy using Dominant Color Descriptor with GaussianNB: 88.75%\n",
    "  \n",
    "    Training and evaluating KNeighborsClassifier...\n",
    "\n",
    "* Accuracy using Dominant Color Descriptor with KNeighborsClassifier: 86.25%\n",
    "  \n",
    "    Training and evaluating LogisticRegression...\n",
    "\n",
    "* Accuracy using Dominant Color Descriptor with LogisticRegression: 77.50%\n",
    "  \n",
    "    Training and evaluating SVC...\n",
    "\n",
    "* Accuracy using Dominant Color Descriptor with SVC: 80.00%\n",
    "  \n",
    "  Training and evaluating MLPClassifier...\n",
    "\n",
    "* Accuracy using Dominant Color Descriptor with MLPClassifier: 65.00%\n",
    "  \n",
    "    Training and evaluating RandomForestClassifier...\n",
    "\n",
    "* Accuracy using Dominant Color Descriptor with RandomForestClassifier: 91.25%\n",
    "  \n",
    "    Training and evaluating GradientBoostingClassifier...\n",
    "  \n",
    "* Accuracy using Dominant Color Descriptor with GradientBoostingClassifier: 92.50%\n",
    "  \n",
    "    Training and evaluating DecisionTreeClassifier...\n",
    "  \n",
    "* Accuracy using Dominant Color Descriptor with DecisionTreeClassifier: 71.25%\n",
    "\n",
    "  The best classifier for HSV Histogram is LogisticRegression with accuracy: 91.25%\n",
    "\n",
    "\n",
    "\n",
    "### 2. Color Coherence Vector\n",
    "\n",
    "* Accuracy using Color Coherence Vector with GaussianNB: 85.75%\n",
    "  \n",
    "    Training and evaluating KNeighborsClassifier...\n",
    "\n",
    "* Accuracy using Color Coherence Vector with KNeighborsClassifier: 82.00%\n",
    "  \n",
    "    Training and evaluating LogisticRegression...\n",
    "\n",
    "* Accuracy using Color Coherence Vector with LogisticRegression: 83.50%\n",
    "  \n",
    "    Training and evaluating SVC...\n",
    "\n",
    "* Accuracy using Color Coherence Vector with SVC: 87.00%\n",
    "  \n",
    "  Training and evaluating MLPClassifier...\n",
    "\n",
    "* Accuracy using Color Coherence Vector with MLPClassifier: 75.50%\n",
    "  \n",
    "    Training and evaluating RandomForestClassifier...\n",
    "\n",
    "* Accuracy using Color Coherence Vector with RandomForestClassifier: 90.75%\n",
    "  \n",
    "    Training and evaluating GradientBoostingClassifier...\n",
    "  \n",
    "* Accuracy using Color Coherence Vector with GradientBoostingClassifier: 97.25%\n",
    "  \n",
    "    Training and evaluating DecisionTreeClassifier...\n",
    "  \n",
    "* Accuracy using Color Coherence Vector with DecisionTreeClassifier: 81.25%\n",
    "\n",
    "  The best classifier for Color Coherence Vector is LogisticRegression with accuracy: 97.25%\n",
    "\n",
    "\n",
    "\n",
    "### 3. HSV Histogram\n",
    "\n",
    "\n",
    "* Training and evaluating GaussianNB...\n",
    "  \n",
    "    Accuracy using HSV Histogram with GaussianNB: 82.50%\n",
    "        \n",
    "* Training and evaluating KNeighborsClassifier...\n",
    "  \n",
    "    Accuracy using HSV Histogram with KNeighborsClassifier: 92.50%\n",
    "        \n",
    "* Training and evaluating LogisticRegression...\n",
    "  \n",
    "    Accuracy using HSV Histogram with LogisticRegression: 95.00%\n",
    "        \n",
    "* Training and evaluating SVC...\n",
    "  \n",
    "    Accuracy using HSV Histogram with SVC: 93.75%\n",
    "        \n",
    "* Training and evaluating MLPClassifier...\n",
    "  \n",
    "    Accuracy using HSV Histogram with MLPClassifier: 92.50%\n",
    "        \n",
    "* Training and evaluating RandomForestClassifier...\n",
    "  \n",
    "    Accuracy using HSV Histogram with RandomForestClassifier: 85.00%\n",
    "        \n",
    "* Training and evaluating GradientBoostingClassifier...\n",
    "  \n",
    "    Accuracy using HSV Histogram with GradientBoostingClassifier: 75.00%\n",
    "        \n",
    "* Training and evaluating DecisionTreeClassifier...\n",
    "  \n",
    "    Accuracy using HSV Histogram with DecisionTreeClassifier: 75.00%\n",
    "\n",
    "  \n",
    "\n",
    "The best classifier for HSV Histogram is LogisticRegression with accuracy: 95.00%\n",
    "\n",
    "\n",
    "\n",
    "### 4. Color Layout Descriptor\n",
    "\n",
    "\n",
    "* Training and evaluating GaussianNB...\n",
    "  \n",
    "    Accuracy using Color Layout Descriptor with GaussianNB: 86.25\n",
    "\n",
    "  %ssianNB: 81.25%\n",
    "\n",
    "        \n",
    "* Training and evaluating KNeighborsClassifier...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with KNeighborsClassifier: 72.50%\n",
    "\n",
    "\n",
    "* Training and evaluating LogisticRegression...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with LogisticRegression: 56.25%\n",
    "\n",
    "\n",
    "* Training and evaluating SVC...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with SVC: 76.25%\n",
    "\n",
    "\n",
    "* Training and evaluating MLPClassifier...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with MLPClassifier: 55.00%\n",
    "\n",
    "\n",
    "* Training and evaluating RandomForestClassifier...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with RandomForestClassifier: 81.25%\n",
    "\n",
    "\n",
    "* Training and evaluating GradientBoostingClassifier...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with GradientBoostingClassifier: 77.50%\n",
    "\n",
    "\n",
    "* Training and evaluating DecisionTreeClassifier...\n",
    "\n",
    "    Accuracy using Color Layout Descriptor with DecisionTreeClassifier: 72.50%\n",
    "\n",
    "\n",
    "\n",
    "The best classifier for Color Layout Descriptor is G6ussianNB with accuracy: 81.25%\n",
    "\n",
    "\n",
    "\n",
    "### 5. Color Name Descriptor\n",
    "\n",
    "* Training and evaluating GaussianNB...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with GaussianNB: 70.00%\n",
    "\n",
    "\n",
    "* Training and evaluating KNeighborsClassifier...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with KNeighborsClassifier: 80.00%\n",
    "\n",
    "\n",
    "* Training and evaluating LogisticRegression...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with LogisticRegression: 77.50%\n",
    "\n",
    "\n",
    "* Training and evaluating SVC...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with SVC: 76.25%\n",
    "\n",
    "\n",
    "* Training and evaluating MLPClassifier...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with MLPClassifier: 80.00%\n",
    "\n",
    "\n",
    "* Training and evaluating RandomForestClassifier...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with RandomForestClassifier: 77.50%\n",
    "\n",
    "\n",
    "* Training and evaluating GradientBoostingClassifier...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with GradientBoostingClassifier: 75.00%\n",
    "    \n",
    "\n",
    "* Training and evaluating DecisionTreeClassifier...\n",
    "\n",
    "    Accuracy using Color Name Descriptor with DecisionTreeClassifier: 76.25%\n",
    "\n",
    "The best classifier for Color Name Descriptor is KNeighborsClassifier with accuracy: 80.00%\n",
    "\n",
    "\n",
    "### 6. Opponency Color Features\n",
    "\n",
    "* Training and evaluating GaussianNB...\n",
    "\n",
    "    Accuracy using Opponency Color Features with GaussianNB: 62.50%\n",
    "\n",
    "\n",
    "* Training and evaluating KNeighborsClassifier...\n",
    "\n",
    "    Accuracy using Opponency Color Features with KNeighborsClassifier: 45.00%\n",
    "        \n",
    "* Training and evaluating LogisticRegression...\n",
    "\n",
    "    Accuracy using Opponency Color Features with LogisticRegression: 30.00%\n",
    "        \n",
    "* Training and evaluating SVC...\n",
    "\n",
    "    Accuracy using Opponency Color Features with SVC: 47.50%\n",
    "        \n",
    "* Training and evaluating MLPClassifier...\n",
    "\n",
    "    Accuracy using Opponency Color Features with MLPClassifier: 32.50%\n",
    "        \n",
    "* Training and evaluating RandomForestClassifier...\n",
    "\n",
    "    Accuracy using Opponency Color Features with RandomForestClassifier: 65.00%\n",
    "\n",
    "\n",
    "* Training and evaluating GradientBoostingClassifier...\n",
    "\n",
    "    Accuracy using Opponency Color Features with GradientBoostingClassifier: 72.00%\n",
    "\n",
    "* Training and evaluating DecisionTreeClassifier...\n",
    "\n",
    "    Accuracy using Opponency Color Features with DecisionTreeClassifier: 70.25%\n",
    "\n",
    "The best classifier for Opponency Color Features is GradientBoostingClassifier with accuracy: 72.00%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d74f4-0720-416e-9335-b73803f9ee0b",
   "metadata": {},
   "source": [
    ".\r\n",
    "\r\n",
    "### Accuracy Explanation\r\n",
    "\r\n",
    "1. **Dominant Color Descriptor**\r\n",
    "   - **Gradient Boosting Classifier (92.50%)**: This method is effective because it combines multiple weak learners to improve performance, which works well with complex features like dominant colors.\r\n",
    "   - **Random Forest Classifier (91.25%)**: Similar to Gradient Boosting but with simpler decision trees; it captures patterns well in dominant color features.\r\n",
    "   - **GaussianNB (88.75%)**: Performs reasonably well, but might struggle with the multi-dimensional color features compared to ensemble methods.\r\n",
    "   - **Other classifiers**: Generally lower due to less effective handling of the dominant color feature set.\r\n",
    "\r\n",
    "2. **Color Coherence Vector**\r\n",
    "   - **Gradient Boosting Classifier (97.25%)**: Excels due to its robustness in handling complex, high-dimensional features like color coherence.\r\n",
    "   - **KNeighborsClassifier (82.00%)**: Works well with local feature patterns but less effective with high-dimensional data compared to boosting methods.\r\n",
    "   - **GaussianNB (85.75%)**: Performs decently but might be limited by assumptions about feature distribution.\r\n",
    "   - **Other classifiers**: Generally less effective due to their inability to capture the complex relationships in coherence features.\r\n",
    "\r\n",
    "3. **HSV Histogram**\r\n",
    "   - **Logistic Regression (95.00%)**: Effective in handling the HSV histogram data, providing high accuracy by finding a good linear decision boundary.\r\n",
    "   - **KNeighborsClassifier (92.50%)**: Good at capturing local patterns in the histogram but not as strong as Logistic Regression.\r\n",
    "   - **GaussianNB (82.50%)**: Adequate but less effective in the high-dimensional HSV space.\r\n",
    "   - **Other classifiers**: Show varied performance, often struggling with the complexity of the HSV histogram features.\r\n",
    "\r\n",
    "4. **Color Layout Descriptor**\r\n",
    "   - **GaussianNB (81.25%)**: Performs well with the average color information but may not capture more complex patterns.\r\n",
    "   - **Random Forest Classifier (81.25%)**: Handles average color information well but might not capture detailed patterns.\r\n",
    "   - **Other classifiers**: Generally perform lower due to less effective handling of color layout features which may lack detailed discrimination.\r\n",
    "\r\n",
    "5. **Color Name Descriptor**\r\n",
    "   - **KNeighborsClassifier (80.00%)**: Good at handling discrete color names and their frequencies, making it effective for this feature set.\r\n",
    "   - **MLPClassifier (80.00%)**: Performs similarly by learning complex relationships between color names and categories.\r\n",
    "   - **GaussianNB (70.00%)**: Less effective due to assumptions about feature distribution and the discrete nature of color names.\r\n",
    "   - **Other classifiers**: Lower performance due to less effective handling of the color name feature distribution.\r\n",
    "\r\n",
    "6. **Opponency Color Features**\r\n",
    "   - **Gradient Boosting Classifier (72.00%)**: Handles the opponency features well by combining multiple weak learners.\r\n",
    "   - **Random Forest Classifier (65.00%)**: Better than most but still struggles with complex opponency features.\r\n",
    "   - **GaussianNB (62.50%)**: Performs the worst as it may not capture the nuances of color opponency effectively.\r\n",
    "   - **Other classifiers**: Struggle with the unique nature of opponency features, leading to lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c245a4-0510-4887-95c7-4b1fc9734cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c37aa-37bc-4eb4-8cae-e94fcadeabf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
